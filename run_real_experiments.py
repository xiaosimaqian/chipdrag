#!/usr/bin/env python3
"""
åŸºäºçœŸå®æ•°æ®çš„å®Œæ•´å®éªŒç³»ç»Ÿ
æ•´åˆRLè®­ç»ƒç»“æœã€å®éªŒè¯„ä¼°æ•°æ®å’ŒåŸºå‡†æµ‹è¯•æ•°æ®
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple
import logging

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def to_serializable(obj):
    if isinstance(obj, dict):
        return {k: to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_serializable(v) for v in obj]
    elif isinstance(obj, np.generic):
        return obj.item()
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    else:
        return obj

class RealDataExperimentSystem:
    """åŸºäºçœŸå®æ•°æ®çš„å®éªŒç³»ç»Ÿ"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data"
        self.results_dir = self.base_dir / "results"
        self.rl_training_dir = self.data_dir / "designs/ispd_2015_contest_benchmark/mgc_des_perf_1/rl_training"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = self.base_dir / "real_experiment_output"
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info(f"å®éªŒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_real_rl_data(self) -> Dict[str, Any]:
        """åŠ è½½çœŸå®çš„RLè®­ç»ƒæ•°æ®"""
        logger.info("åŠ è½½çœŸå®RLè®­ç»ƒæ•°æ®...")
        
        # åŠ è½½è®­ç»ƒå†å²
        training_history = pd.read_csv(self.rl_training_dir / "training_history.csv")
        
        # åŠ è½½episodeæ•°æ®
        episodes = {}
        for i in range(1, 6):  # 5ä¸ªepisode
            episode_file = self.rl_training_dir / f"episode_{i}.json"
            if episode_file.exists():
                with open(episode_file, 'r') as f:
                    episodes[f"episode_{i}"] = json.load(f)
        
        # åˆ†æè®­ç»ƒæ›²çº¿
        training_analysis = {
            'total_episodes': len(training_history),
            'avg_reward': float(training_history['total_reward'].mean()),
            'reward_std': float(training_history['total_reward'].std()),
            'best_episode': training_history.loc[training_history['total_reward'].idxmax()].to_dict(),
            'worst_episode': training_history.loc[training_history['total_reward'].idxmin()].to_dict(),
            'convergence_analysis': self._analyze_convergence(training_history),
            'hpwl_improvement': self._analyze_hpwl_improvement(training_history),
            'overflow_analysis': self._analyze_overflow(training_history)
        }
        
        return {
            'training_history': training_history.to_dict('records'),
            'episodes': episodes,
            'analysis': training_analysis
        }
    
    def load_real_experiment_results(self) -> Dict[str, Any]:
        """åŠ è½½çœŸå®çš„å®éªŒè¯„ä¼°ç»“æœ"""
        logger.info("åŠ è½½çœŸå®å®éªŒè¯„ä¼°ç»“æœ...")
        
        # æ‰¾åˆ°æœ€æ–°çš„å®éªŒç»“æœ
        result_files = list(self.results_dir.glob("experiment_results_*.json"))
        if not result_files:
            logger.warning("æœªæ‰¾åˆ°å®éªŒç»“æœæ–‡ä»¶")
            return {}
        
        # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        latest_result = max(result_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ä½¿ç”¨æœ€æ–°å®éªŒç»“æœ: {latest_result.name}")
        
        with open(latest_result, 'r') as f:
            experiment_data = json.load(f)
        
        # åŠ è½½å®éªŒæ‘˜è¦
        summary_file = self.results_dir / "experiment_summary.csv"
        if summary_file.exists():
            summary_data = pd.read_csv(summary_file)
        else:
            summary_data = pd.DataFrame()
        
        return {
            'experiment_data': experiment_data,
            'summary_data': summary_data.to_dict('records') if not summary_data.empty else [],
            'file_info': {
                'latest_result': latest_result.name,
                'total_results': len(result_files)
            }
        }
    
    def load_benchmark_data(self) -> Dict[str, Any]:
        """åŠ è½½åŸºå‡†æµ‹è¯•æ•°æ®"""
        logger.info("åŠ è½½åŸºå‡†æµ‹è¯•æ•°æ®...")
        
        benchmark_dir = self.data_dir / "designs/ispd_2015_contest_benchmark"
        benchmarks = {}
        
        for benchmark in ['mgc_des_perf_1', 'mgc_fft_1', 'mgc_pci_bridge32_a']:
            benchmark_path = benchmark_dir / benchmark
            if benchmark_path.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰DEFæ–‡ä»¶
                def_files = list(benchmark_path.glob("*.def"))
                lib_files = list(benchmark_path.glob("*.lib"))
                lef_files = list(benchmark_path.glob("*.lef"))
                
                benchmarks[benchmark] = {
                    'def_files': [f.name for f in def_files],
                    'lib_files': [f.name for f in lib_files],
                    'lef_files': [f.name for f in lef_files],
                    'has_rl_training': (benchmark_path / "rl_training").exists(),
                    'total_files': len(def_files) + len(lib_files) + len(lef_files)
                }
        
        return benchmarks
    
    def _analyze_convergence(self, training_history: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè®­ç»ƒæ”¶æ•›æ€§"""
        rewards = training_history['total_reward'].values
        
        # è®¡ç®—æ”¶æ•›æŒ‡æ ‡
        convergence = {
            'reward_trend': 'increasing' if rewards[-1] > rewards[0] else 'decreasing',
            'reward_variance': np.var(rewards),
            'final_reward': rewards[-1],
            'initial_reward': rewards[0],
            'improvement_ratio': (rewards[-1] - rewards[0]) / abs(rewards[0]) if rewards[0] != 0 else 0
        }
        
        return convergence
    
    def _analyze_hpwl_improvement(self, training_history: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æHPWLæ”¹è¿›æƒ…å†µ"""
        hpwl_values = training_history['final_hpwl'].values
        
        improvement = {
            'initial_hpwl': hpwl_values[0],
            'final_hpwl': hpwl_values[-1],
            'best_hpwl': hpwl_values.min(),
            'hpwl_reduction': (hpwl_values[0] - hpwl_values[-1]) / hpwl_values[0] * 100,
            'hpwl_std': np.std(hpwl_values)
        }
        
        return improvement
    
    def _analyze_overflow(self, training_history: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æoverflowæƒ…å†µ"""
        overflow_values = training_history['final_overflow'].values
        
        overflow_analysis = {
            'initial_overflow': overflow_values[0],
            'final_overflow': overflow_values[-1],
            'best_overflow': overflow_values.min(),
            'overflow_improvement': (overflow_values[0] - overflow_values[-1]) / overflow_values[0] * 100 if overflow_values[0] != 0 else 0,
            'overflow_std': np.std(overflow_values)
        }
        
        return overflow_analysis
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š...")
        
        # åŠ è½½æ‰€æœ‰çœŸå®æ•°æ®
        rl_data = self.load_real_rl_data()
        experiment_data = self.load_real_experiment_results()
        benchmark_data = self.load_benchmark_data()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'experiment_summary': {
                'rl_training_episodes': len(rl_data.get('training_history', [])),
                'experiment_results_count': experiment_data.get('file_info', {}).get('total_results', 0),
                'available_benchmarks': len(benchmark_data),
                'total_benchmark_files': sum(b['total_files'] for b in benchmark_data.values())
            },
            'rl_training_analysis': rl_data.get('analysis', {}),
            'experiment_performance': self._analyze_experiment_performance(experiment_data),
            'benchmark_coverage': benchmark_data,
            'system_capabilities': self._assess_system_capabilities(rl_data, experiment_data, benchmark_data)
        }
        
        # ä¿å­˜æŠ¥å‘Šï¼ˆä¿®å¤åºåˆ—åŒ–é—®é¢˜ï¼‰
        report_serializable = to_serializable(report)
        report_file = self.output_dir / "comprehensive_experiment_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_serializable, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report
    
    def _analyze_experiment_performance(self, experiment_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå®éªŒæ€§èƒ½"""
        summary_data = experiment_data.get('summary_data', [])
        
        if not summary_data:
            return {'status': 'no_data_available'}
        
        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œåˆ†æ
        df = pd.DataFrame(summary_data)
        
        performance = {
            'total_cases': len(df),
            'feasible_rate': (df['feasible'] == True).mean() * 100 if 'feasible' in df.columns else 0,
            'avg_timing_margin': df['timing_margin'].mean() if 'timing_margin' in df.columns else 0,
            'avg_area_utilization': df['area'].mean() if 'area' in df.columns else 0,
            'avg_power_consumption': df['power'].mean() if 'power' in df.columns else 0,
            'avg_overall_score': df['score'].mean() if 'score' in df.columns else 0,
            'avg_knowledge_reuse': df['reuse_rate'].mean() if 'reuse_rate' in df.columns else 0
        }
        
        return performance
    
    def _assess_system_capabilities(self, rl_data: Dict, experiment_data: Dict, benchmark_data: Dict) -> Dict[str, Any]:
        """è¯„ä¼°ç³»ç»Ÿèƒ½åŠ›"""
        capabilities = {
            'rl_training': {
                'implemented': bool(rl_data.get('training_history')),
                'episodes_completed': len(rl_data.get('training_history', [])),
                'convergence_achieved': rl_data.get('analysis', {}).get('convergence_analysis', {}).get('improvement_ratio', 0) > 0
            },
            'experiment_evaluation': {
                'implemented': bool(experiment_data.get('experiment_data')),
                'results_available': experiment_data.get('file_info', {}).get('total_results', 0) > 0,
                'performance_metrics': bool(experiment_data.get('summary_data'))
            },
            'benchmark_support': {
                'benchmarks_available': len(benchmark_data),
                'design_files_present': sum(b['total_files'] for b in benchmark_data.values()) > 0,
                'rl_training_data': any(b.get('has_rl_training') for b in benchmark_data.values())
            },
            'overall_readiness': {
                'ready_for_paper': self._assess_paper_readiness(rl_data, experiment_data, benchmark_data),
                'missing_components': self._identify_missing_components(rl_data, experiment_data, benchmark_data)
            }
        }
        
        return capabilities
    
    def _assess_paper_readiness(self, rl_data: Dict, experiment_data: Dict, benchmark_data: Dict) -> Dict[str, Any]:
        """è¯„ä¼°è®ºæ–‡å‡†å¤‡å°±ç»ªç¨‹åº¦"""
        readiness = {
            'rl_training_results': bool(rl_data.get('training_history')),
            'experiment_evaluation': bool(experiment_data.get('experiment_data')),
            'benchmark_coverage': len(benchmark_data) >= 3,  # è‡³å°‘3ä¸ªåŸºå‡†æµ‹è¯•
            'performance_metrics': bool(experiment_data.get('summary_data')),
            'convergence_analysis': bool(rl_data.get('analysis', {}).get('convergence_analysis'))
        }
        
        # è®¡ç®—æ€»ä½“å°±ç»ªåº¦
        readiness['overall_score'] = sum(readiness.values()) / len(readiness) * 100
        
        return readiness
    
    def _identify_missing_components(self, rl_data: Dict, experiment_data: Dict, benchmark_data: Dict) -> List[str]:
        """è¯†åˆ«ç¼ºå¤±çš„ç»„ä»¶"""
        missing = []
        
        if not rl_data.get('training_history'):
            missing.append("RLè®­ç»ƒå†å²æ•°æ®")
        
        if not experiment_data.get('experiment_data'):
            missing.append("å®éªŒè¯„ä¼°ç»“æœ")
        
        if len(benchmark_data) < 3:
            missing.append("è¶³å¤Ÿçš„åŸºå‡†æµ‹è¯•æ•°æ®")
        
        if not experiment_data.get('summary_data'):
            missing.append("å®éªŒæ€§èƒ½æ‘˜è¦")
        
        return missing
    
    def generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # åŠ è½½æ•°æ®
        rl_data = self.load_real_rl_data()
        experiment_data = self.load_real_experiment_results()
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Chip-D-RAGç³»ç»ŸçœŸå®å®éªŒæ•°æ®å¯è§†åŒ–', fontsize=16, fontweight='bold')
        
        # 1. RLè®­ç»ƒå¥–åŠ±æ›²çº¿
        if rl_data.get('training_history'):
            training_df = pd.DataFrame(rl_data['training_history'])
            axes[0, 0].plot(training_df['episode'], training_df['total_reward'], 'b-o', linewidth=2, markersize=6)
            axes[0, 0].set_title('RLè®­ç»ƒå¥–åŠ±æ›²çº¿')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('æ€»å¥–åŠ±')
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. HPWLæ”¹è¿›æƒ…å†µ
        if rl_data.get('training_history'):
            training_df = pd.DataFrame(rl_data['training_history'])
            axes[0, 1].plot(training_df['episode'], training_df['final_hpwl'], 'r-s', linewidth=2, markersize=6)
            axes[0, 1].set_title('HPWLä¼˜åŒ–æƒ…å†µ')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('æœ€ç»ˆHPWL')
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å®éªŒæ€§èƒ½æŒ‡æ ‡
        if experiment_data.get('summary_data'):
            summary_df = pd.DataFrame(experiment_data['summary_data'])
            if 'score' in summary_df.columns:
                axes[1, 0].bar(summary_df['case'], summary_df['score'], color='green', alpha=0.7)
                axes[1, 0].set_title('å„æ¡ˆä¾‹æ€»ä½“è¯„åˆ†')
                axes[1, 0].set_xlabel('æµ‹è¯•æ¡ˆä¾‹')
                axes[1, 0].set_ylabel('è¯„åˆ†')
                axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. çº¦æŸæ»¡è¶³æƒ…å†µ
        if experiment_data.get('summary_data'):
            summary_df = pd.DataFrame(experiment_data['summary_data'])
            if 'timing_margin' in summary_df.columns and 'area' in summary_df.columns:
                axes[1, 1].scatter(summary_df['area'], summary_df['timing_margin'], 
                                 s=100, c='purple', alpha=0.7)
                axes[1, 1].set_title('é¢ç§¯åˆ©ç”¨ç‡ vs æ—¶åºè£•é‡')
                axes[1, 1].set_xlabel('é¢ç§¯åˆ©ç”¨ç‡')
                axes[1, 1].set_ylabel('æ—¶åºè£•é‡')
                axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_file = self.output_dir / "experiment_visualizations.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {chart_file}")
        
        plt.show()
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info("å¼€å§‹è¿è¡Œå®Œæ•´å®éªŒåˆ†æ...")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report()
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.generate_visualizations()
        
        # æ‰“å°å…³é”®ç»“æœ
        self._print_key_results(report)
        
        logger.info("å®Œæ•´å®éªŒåˆ†æå®Œæˆï¼")
        return report
    
    def _print_key_results(self, report: Dict[str, Any]):
        """æ‰“å°å…³é”®ç»“æœ"""
        print("\n" + "="*60)
        print("Chip-D-RAGç³»ç»ŸçœŸå®å®éªŒæ•°æ®å…³é”®ç»“æœ")
        print("="*60)
        
        # RLè®­ç»ƒç»“æœ
        rl_analysis = report.get('rl_training_analysis', {})
        if rl_analysis:
            print(f"\nğŸ”§ RLè®­ç»ƒç»“æœ:")
            print(f"   - è®­ç»ƒè½®æ•°: {report['experiment_summary']['rl_training_episodes']}")
            print(f"   - å¹³å‡å¥–åŠ±: {rl_analysis.get('avg_reward', 0):.2f}")
            print(f"   - HPWLæ”¹è¿›: {rl_analysis.get('hpwl_improvement', {}).get('hpwl_reduction', 0):.1f}%")
            print(f"   - æ”¶æ•›çŠ¶æ€: {rl_analysis.get('convergence_analysis', {}).get('reward_trend', 'unknown')}")
        
        # å®éªŒæ€§èƒ½
        exp_performance = report.get('experiment_performance', {})
        if exp_performance:
            print(f"\nğŸ“Š å®éªŒæ€§èƒ½:")
            print(f"   - æµ‹è¯•æ¡ˆä¾‹æ•°: {exp_performance.get('total_cases', 0)}")
            print(f"   - å¯è¡Œç‡: {exp_performance.get('feasible_rate', 0):.1f}%")
            print(f"   - å¹³å‡æ—¶åºè£•é‡: {exp_performance.get('avg_timing_margin', 0):.3f}")
            print(f"   - å¹³å‡æ€»ä½“è¯„åˆ†: {exp_performance.get('avg_overall_score', 0):.3f}")
        
        # ç³»ç»Ÿèƒ½åŠ›
        capabilities = report.get('system_capabilities', {})
        if capabilities:
            readiness = capabilities.get('overall_readiness', {})
            print(f"\nğŸ¯ ç³»ç»Ÿå°±ç»ªåº¦:")
            print(f"   - æ€»ä½“å°±ç»ªåº¦: {readiness.get('overall_score', 0):.1f}%")
            print(f"   - åŸºå‡†æµ‹è¯•è¦†ç›–: {len(report.get('benchmark_coverage', {}))}")
            print(f"   - å®éªŒç»“æœå¯ç”¨: {capabilities.get('experiment_evaluation', {}).get('results_available', 0)}")
        
        # ç¼ºå¤±ç»„ä»¶
        missing = capabilities.get('overall_readiness', {}).get('missing_components', [])
        if missing:
            print(f"\nâš ï¸  ç¼ºå¤±ç»„ä»¶:")
            for component in missing:
                print(f"   - {component}")
        
        print("\n" + "="*60)

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå®éªŒç³»ç»Ÿ
    experiment_system = RealDataExperimentSystem()
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    report = experiment_system.run_complete_analysis()
    
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {experiment_system.output_dir}")
    print("ğŸ“„ è¯¦ç»†æŠ¥å‘Š: comprehensive_experiment_report.json")
    print("ğŸ“Š å¯è§†åŒ–å›¾è¡¨: experiment_visualizations.png")

if __name__ == "__main__":
    main() 